import axios from 'axios';\nimport { performance } from 'perf_hooks';\nimport { EventEmitter } from 'events';\nimport { piiSafeLogger } from '../../utils/monitoring/piiSafeLogging.mjs';\nimport { mcpHealthManager } from '../../utils/monitoring/mcpHealthManager.mjs';\nimport sequelize from '../../database.mjs';\n\n/**\n * P2: Advanced MCP Analytics and Monitoring\n * Deep analytics for MCP server performance, token usage, and quality metrics\n * Aligned with Master Prompt v26 MCP-Centric Architecture\n */\n\nclass MCPAnalytics extends EventEmitter {\n  constructor() {\n    super();\n    \n    this.analyticsDB = sequelize; // Use main database for analytics\n    this.tokenCosts = {\n      'claude-3-5-sonnet': { input: 0.003, output: 0.015 }, // per 1K tokens\n      'claude-3-opus': { input: 0.015, output: 0.075 },\n      'gpt-4': { input: 0.01, output: 0.03 },\n      'gpt-3.5-turbo': { input: 0.0005, output: 0.0015 }\n    };\n    \n    this.qualityMetrics = {\n      responseTime: { weight: 0.25, threshold: 3000 }, // ms\n      accuracy: { weight: 0.25, threshold: 0.85 }, // 0-1\n      completion: { weight: 0.25, threshold: 0.95 }, // 0-1\n      userSatisfaction: { weight: 0.25, threshold: 4.0 } // 1-5\n    };\n    \n    this.alertThresholds = {\n      tokenUsageSpike: 2.0, // 2x normal usage\n      responseTimeDegradation: 1.5, // 1.5x normal response time\n      errorRateIncrease: 0.1, // 10% error rate\n      costThreshold: 100, // $100 per hour\n      qualityDegradation: 0.8 // Quality score below 80%\n    };\n    \n    // Initialize metrics tracking\n    this.metricsHistory = new Map();\n    this.realTimeMetrics = new Map();\n    \n    // Start real-time monitoring\n    this.startRealTimeMonitoring();\n  }\n\n  /**\n   * Track token usage for MCP operations\n   * @param {string} server - MCP server name\n   * @param {string} operation - Operation performed\n   * @param {number} tokens - Number of tokens used\n   * @param {string} model - AI model used\n   */\n  async trackTokenUsage(server, operation, tokens, model = 'claude-3-5-sonnet') {\n    try {\n      const cost = this.calculateTokenCost(tokens, model);\n      const timestamp = Date.now();\n      \n      // Store in database (if models exist)\n      try {\n        if (this.analyticsDB.models && this.analyticsDB.models.MCPTokenUsage) {\n          await this.analyticsDB.models.MCPTokenUsage.create({\n            server,\n            operation,\n            tokens,\n            model,\n            cost,\n            timestamp: new Date(timestamp)\n          });\n        }\n      } catch (dbError) {\n        // If models don't exist, log the issue but continue\n        piiSafeLogger.warn('MCPTokenUsage model not found, storing in memory only', {\n          error: dbError.message\n        });\n      }\n      \n      // Update real-time metrics\n      this.updateRealTimeMetrics(server, {\n        tokenUsage: tokens,\n        cost,\n        operation\n      });\n      \n      // Check for token usage spikes\n      await this.checkTokenUsageAlert(server, operation, tokens);\n      \n      // Log with PII safety\n      piiSafeLogger.trackMCPOperation(server, 'token_usage_tracked', {\n        operation,\n        tokens,\n        cost,\n        model\n      });\n      \n      this.emit('tokenUsage', {\n        server,\n        operation,\n        tokens,\n        cost,\n        model,\n        timestamp\n      });\n    } catch (error) {\n      piiSafeLogger.error('Failed to track token usage', {\n        error: error.message,\n        server,\n        operation,\n        tokens\n      });\n    }\n  }\n\n  /**\n   * Calculate token cost based on model pricing\n   * @param {number} tokens - Number of tokens\n   * @param {string} model - AI model used\n   * @param {string} type - 'input' or 'output'\n   */\n  calculateTokenCost(tokens, model, type = 'output') {\n    const modelPricing = this.tokenCosts[model] || this.tokenCosts['claude-3-5-sonnet'];\n    const rate = modelPricing[type] || modelPricing.output;\n    return (tokens / 1000) * rate;\n  }\n\n  /**\n   * Track MCP operation quality metrics\n   * @param {string} server - MCP server name\n   * @param {string} operation - Operation performed\n   * @param {Object} metrics - Quality metrics object\n   */\n  async trackQualityMetrics(server, operation, metrics) {\n    try {\n      const qualityRecord = {\n        server,\n        operation,\n        responseTime: metrics.responseTime || 0,\n        accuracy: metrics.accuracy || 0,\n        completion: metrics.completion || 0,\n        userSatisfaction: metrics.userSatisfaction || 0,\n        overallQuality: this.calculateOverallQuality(metrics),\n        timestamp: new Date()\n      };\n      \n      // Store in database (if models exist)\n      try {\n        if (this.analyticsDB.models && this.analyticsDB.models.MCPQualityMetrics) {\n          await this.analyticsDB.models.MCPQualityMetrics.create(qualityRecord);\n        }\n      } catch (dbError) {\n        // If models don't exist, log the issue but continue\n        piiSafeLogger.warn('MCPQualityMetrics model not found, storing in memory only', {\n          error: dbError.message\n        });\n      }\n      \n      // Update real-time metrics\n      this.updateRealTimeMetrics(server, {\n        qualityScore: qualityRecord.overallQuality,\n        responseTime: metrics.responseTime\n      });\n      \n      // Check for quality degradation\n      await this.checkQualityAlert(server, operation, qualityRecord);\n      \n      piiSafeLogger.trackMCPOperation(server, 'quality_tracked', {\n        operation,\n        qualityScore: qualityRecord.overallQuality,\n        responseTime: metrics.responseTime\n      });\n      \n      this.emit('qualityUpdate', {\n        server,\n        operation,\n        ...qualityRecord\n      });\n      \n      return qualityRecord;\n    } catch (error) {\n      piiSafeLogger.error('Failed to track quality metrics', {\n        error: error.message,\n        server,\n        operation\n      });\n      return null;\n    }\n  }\n\n  /**\n   * Calculate overall quality score\n   * @param {Object} metrics - Individual quality metrics\n   */\n  calculateOverallQuality(metrics) {\n    let totalScore = 0;\n    let totalWeight = 0;\n    \n    for (const [metricName, config] of Object.entries(this.qualityMetrics)) {\n      if (metrics[metricName] !== undefined) {\n        let normalizedScore = metrics[metricName];\n        \n        // Normalize response time (lower is better)\n        if (metricName === 'responseTime') {\n          normalizedScore = Math.max(0, 1 - (metrics[metricName] / (config.threshold * 2)));\n        }\n        \n        totalScore += normalizedScore * config.weight;\n        totalWeight += config.weight;\n      }\n    }\n    \n    return totalWeight > 0 ? Math.round((totalScore / totalWeight) * 100) / 100 : 0;\n  }\n\n  /**\n   * Generate comprehensive MCP health report\n   * @param {string} timeframe - 'hour', 'day', 'week', 'month'\n   */\n  async generateMCPHealthReport(timeframe = 'day') {\n    try {\n      const timeframeMs = this.getTimeframeMs(timeframe);\n      const startTime = new Date(Date.now() - timeframeMs);\n      \n      const report = {\n        timeframe,\n        startTime: startTime.toISOString(),\n        endTime: new Date().toISOString(),\n        summary: {\n          serverHealth: await this.getServerHealthSummary(startTime),\n          tokenEfficiency: await this.calculateTokenEfficiency(startTime),\n          responseQuality: await this.analyzeResponseQuality(startTime),\n          userSatisfaction: await this.getUserFeedbackScores(startTime),\n          costAnalysis: await this.analyzeCosts(startTime),\n          alertsSummary: await this.getAlertsSummary(startTime)\n        },\n        serverDetails: {},\n        recommendations: [],\n        trends: {}\n      };\n      \n      // Get detailed metrics for each server\n      const servers = Object.keys(mcpHealthManager.mcpServers);\n      for (const serverKey of servers) {\n        report.serverDetails[serverKey] = await this.getServerDetailedMetrics(serverKey, startTime);\n      }\n      \n      // Analyze trends\n      report.trends = await this.analyzeTrends(startTime);\n      \n      // Generate recommendations\n      report.recommendations = this.generateRecommendations(report);\n      \n      // Log report generation\n      piiSafeLogger.trackMCPOperation('analytics', 'health_report_generated', {\n        timeframe,\n        serverCount: servers.length,\n        totalOperations: report.summary.serverHealth.totalOperations\n      });\n      \n      return report;\n    } catch (error) {\n      piiSafeLogger.error('Failed to generate MCP health report', {\n        error: error.message,\n        timeframe\n      });\n      throw error;\n    }\n  }\n\n  /**\n   * Get server health summary\n   * @param {Date} startTime - Start time for analysis\n   */\n  async getServerHealthSummary(startTime) {\n    try {\n      // Get current health status\n      const ecosystemHealth = await mcpHealthManager.getMCPEcosystemHealth();\n      \n      // Calculate uptime percentage\n      const uptimeStats = await this.calculateUptimeStats(startTime);\n      \n      return {\n        overallHealth: ecosystemHealth.overallHealth,\n        healthyServers: `${ecosystemHealth.healthyServers}/${ecosystemHealth.totalServers}`,\n        averageLatency: ecosystemHealth.averageLatency,\n        uptimePercentage: uptimeStats.uptimePercentage,\n        totalOperations: uptimeStats.totalOperations,\n        successRate: uptimeStats.successRate\n      };\n    } catch (error) {\n      piiSafeLogger.error('Failed to get server health summary', {\n        error: error.message\n      });\n      return {\n        overallHealth: 0,\n        healthyServers: '0/0',\n        averageLatency: 0,\n        uptimePercentage: 0,\n        totalOperations: 0,\n        successRate: 0\n      };\n    }\n  }\n\n  /**\n   * Calculate token efficiency metrics using in-memory data if DB models not available\n   * @param {Date} startTime - Start time for analysis\n   */\n  async calculateTokenEfficiency(startTime) {\n    try {\n      let tokenStats = [];\n      \n      // Try to get from database first\n      try {\n        if (this.analyticsDB.models && this.analyticsDB.models.MCPTokenUsage) {\n          tokenStats = await this.analyticsDB.models.MCPTokenUsage.findAll({\n            where: {\n              timestamp: {\n                [sequelize.Op.gte]: startTime\n              }\n            },\n            attributes: [\n              'server',\n              [sequelize.fn('SUM', sequelize.col('tokens')), 'totalTokens'],\n              [sequelize.fn('SUM', sequelize.col('cost')), 'totalCost'],\n              [sequelize.fn('AVG', sequelize.col('tokens')), 'avgTokens'],\n              [sequelize.fn('COUNT', sequelize.col('id')), 'operationCount']\n            ],\n            group: ['server']\n          });\n        }\n      } catch (dbError) {\n        // Fall back to in-memory data\n        piiSafeLogger.info('Using in-memory data for token efficiency calculation');\n      }\n      \n      const efficiency = {\n        totalTokensUsed: 0,\n        totalCost: 0,\n        averageTokensPerOperation: 0,\n        costPerOperation: 0,\n        tokenEfficiencyScore: 100,\n        serverBreakdown: {}\n      };\n      \n      // Process database results if available\n      if (tokenStats.length > 0) {\n        for (const stat of tokenStats) {\n          const server = stat.get('server');\n          const totalTokens = parseFloat(stat.get('totalTokens')) || 0;\n          const totalCost = parseFloat(stat.get('totalCost')) || 0;\n          const avgTokens = parseFloat(stat.get('avgTokens')) || 0;\n          const operationCount = parseInt(stat.get('operationCount')) || 0;\n          \n          efficiency.totalTokensUsed += totalTokens;\n          efficiency.totalCost += totalCost;\n          \n          efficiency.serverBreakdown[server] = {\n            totalTokens,\n            totalCost,\n            avgTokens,\n            operationCount,\n            costPerOperation: operationCount > 0 ? totalCost / operationCount : 0\n          };\n        }\n      } else {\n        // Use real-time metrics as fallback\n        for (const [server, metrics] of this.realTimeMetrics.entries()) {\n          efficiency.totalTokensUsed += metrics.tokenUsage || 0;\n          efficiency.totalCost += metrics.cost || 0;\n          efficiency.serverBreakdown[server] = {\n            totalTokens: metrics.tokenUsage || 0,\n            totalCost: metrics.cost || 0,\n            avgTokens: metrics.operationCount > 0 ? metrics.tokenUsage / metrics.operationCount : 0,\n            operationCount: metrics.operationCount || 0,\n            costPerOperation: metrics.operationCount > 0 ? metrics.cost / metrics.operationCount : 0\n          };\n        }\n      }\n      \n      // Calculate overall averages\n      const totalOperations = Object.values(efficiency.serverBreakdown)\n        .reduce((sum, server) => sum + server.operationCount, 0);\n      \n      efficiency.averageTokensPerOperation = totalOperations > 0 \n        ? efficiency.totalTokensUsed / totalOperations \n        : 0;\n      efficiency.costPerOperation = totalOperations > 0 \n        ? efficiency.totalCost / totalOperations \n        : 0;\n      \n      // Calculate efficiency score (based on cost per token)\n      const targetCostPerToken = 0.015; // Target cost in dollars\n      const actualCostPerToken = efficiency.totalTokensUsed > 0 \n        ? efficiency.totalCost / efficiency.totalTokensUsed * 1000 // Convert to per 1K tokens\n        : 0;\n      \n      efficiency.tokenEfficiencyScore = Math.max(0, \n        Math.min(100, actualCostPerToken > 0 ? (targetCostPerToken / actualCostPerToken) * 100 : 100)\n      );\n      \n      return efficiency;\n    } catch (error) {\n      piiSafeLogger.error('Failed to calculate token efficiency', {\n        error: error.message\n      });\n      return {\n        totalTokensUsed: 0,\n        totalCost: 0,\n        averageTokensPerOperation: 0,\n        costPerOperation: 0,\n        tokenEfficiencyScore: 0,\n        serverBreakdown: {}\n      };\n    }\n  }\n\n  /**\n   * Analyze response quality across servers\n   * @param {Date} startTime - Start time for analysis\n   */\n  async analyzeResponseQuality(startTime) {\n    try {\n      let qualityStats = [];\n      \n      // Try to get from database first\n      try {\n        if (this.analyticsDB.models && this.analyticsDB.models.MCPQualityMetrics) {\n          qualityStats = await this.analyticsDB.models.MCPQualityMetrics.findAll({\n            where: {\n              timestamp: {\n                [sequelize.Op.gte]: startTime\n              }\n            },\n            attributes: [\n              'server',\n              [sequelize.fn('AVG', sequelize.col('overallQuality')), 'avgQuality'],\n              [sequelize.fn('AVG', sequelize.col('responseTime')), 'avgResponseTime'],\n              [sequelize.fn('AVG', sequelize.col('accuracy')), 'avgAccuracy'],\n              [sequelize.fn('AVG', sequelize.col('completion')), 'avgCompletion'],\n              [sequelize.fn('COUNT', sequelize.col('id')), 'sampleSize']\n            ],\n            group: ['server']\n          });\n        }\n      } catch (dbError) {\n        // Fall back to in-memory data\n        piiSafeLogger.info('Using in-memory data for quality analysis');\n      }\n      \n      const quality = {\n        overallQualityScore: 0,\n        averageResponseTime: 0,\n        accuracyScore: 0,\n        completionScore: 0,\n        serverBreakdown: {},\n        trends: {\n          qualityImproving: false,\n          responseTimeImproving: false\n        }\n      };\n      \n      let totalSamples = 0;\n      let weightedQualitySum = 0;\n      let weightedResponseTimeSum = 0;\n      let weightedAccuracySum = 0;\n      let weightedCompletionSum = 0;\n      \n      if (qualityStats.length > 0) {\n        // Process database results\n        for (const stat of qualityStats) {\n          const server = stat.get('server');\n          const avgQuality = parseFloat(stat.get('avgQuality')) || 0;\n          const avgResponseTime = parseFloat(stat.get('avgResponseTime')) || 0;\n          const avgAccuracy = parseFloat(stat.get('avgAccuracy')) || 0;\n          const avgCompletion = parseFloat(stat.get('avgCompletion')) || 0;\n          const sampleSize = parseInt(stat.get('sampleSize')) || 0;\n          \n          // Weight by sample size\n          weightedQualitySum += avgQuality * sampleSize;\n          weightedResponseTimeSum += avgResponseTime * sampleSize;\n          weightedAccuracySum += avgAccuracy * sampleSize;\n          weightedCompletionSum += avgCompletion * sampleSize;\n          totalSamples += sampleSize;\n          \n          quality.serverBreakdown[server] = {\n            qualityScore: avgQuality,\n            responseTime: avgResponseTime,\n            accuracy: avgAccuracy,\n            completion: avgCompletion,\n            sampleSize\n          };\n        }\n      } else {\n        // Use real-time metrics as fallback\n        for (const [server, metrics] of this.realTimeMetrics.entries()) {\n          const sampleSize = metrics.operationCount || 1;\n          weightedQualitySum += (metrics.qualityScore || 0.8) * sampleSize;\n          weightedResponseTimeSum += (metrics.responseTime || 1000) * sampleSize;\n          totalSamples += sampleSize;\n          \n          quality.serverBreakdown[server] = {\n            qualityScore: metrics.qualityScore || 0.8,\n            responseTime: metrics.responseTime || 1000,\n            accuracy: 0.85, // Default values for fallback\n            completion: 0.9,\n            sampleSize\n          };\n        }\n      }\n      \n      // Calculate weighted averages\n      if (totalSamples > 0) {\n        quality.overallQualityScore = weightedQualitySum / totalSamples;\n        quality.averageResponseTime = weightedResponseTimeSum / totalSamples;\n        quality.accuracyScore = weightedAccuracySum / totalSamples;\n        quality.completionScore = weightedCompletionSum / totalSamples;\n      }\n      \n      // Analyze trends (compare with previous period)\n      quality.trends = await this.analyzeQualityTrends(startTime);\n      \n      return quality;\n    } catch (error) {\n      piiSafeLogger.error('Failed to analyze response quality', {\n        error: error.message\n      });\n      return {\n        overallQualityScore: 0,\n        averageResponseTime: 0,\n        accuracyScore: 0,\n        completionScore: 0,\n        serverBreakdown: {},\n        trends: {\n          qualityImproving: false,\n          responseTimeImproving: false\n        }\n      };\n    }\n  }\n\n  /**\n   * Get user feedback scores\n   * @param {Date} startTime - Start time for analysis\n   */\n  async getUserFeedbackScores(startTime) {\n    try {\n      // This would integrate with a feedback system\n      // For now, returning mock data structure with some variability\n      const baseRating = 4.2;\n      const variance = (Math.random() - 0.5) * 0.4; // +/- 0.2 variance\n      const currentRating = Math.max(1, Math.min(5, baseRating + variance));\n      \n      return {\n        averageRating: Math.round(currentRating * 10) / 10,\n        totalFeedbacks: Math.floor(140 + Math.random() * 30),\n        distributionByRating: {\n          5: Math.floor(50 + Math.random() * 20),\n          4: Math.floor(35 + Math.random() * 15),\n          3: Math.floor(20 + Math.random() * 15),\n          2: Math.floor(8 + Math.random() * 8),\n          1: Math.floor(3 + Math.random() * 5)\n        },\n        sentimentAnalysis: {\n          positive: 0.65 + Math.random() * 0.15,\n          neutral: 0.20 + Math.random() * 0.10,\n          negative: 0.08 + Math.random() * 0.08\n        },\n        topCompliments: [\n          'Fast response time',\n          'Accurate results',\n          'Easy to use',\n          'Helpful recommendations'\n        ],\n        topComplaints: [\n          'Occasional slow response',\n          'Could be more detailed',\n          'Sometimes misunderstands context'\n        ]\n      };\n    } catch (error) {\n      piiSafeLogger.error('Failed to get user feedback scores', {\n        error: error.message\n      });\n      return {\n        averageRating: 0,\n        totalFeedbacks: 0,\n        distributionByRating: {},\n        sentimentAnalysis: { positive: 0, neutral: 0, negative: 0 },\n        topCompliments: [],\n        topComplaints: []\n      };\n    }\n  }\n\n  /**\n   * Analyze costs across timeframe\n   * @param {Date} startTime - Start time for analysis\n   */\n  async analyzeCosts(startTime) {\n    try {\n      const analysis = {\n        totalCost: 0,\n        costByServer: {},\n        costByModel: {},\n        costTrends: {\n          increasing: false,\n          percentageChange: 0\n        },\n        costOptimizationOpportunities: [],\n        projectedMonthlyCost: 0\n      };\n      \n      // Use real-time metrics for cost calculation\n      for (const [server, metrics] of this.realTimeMetrics.entries()) {\n        const serverCost = metrics.cost || 0;\n        analysis.totalCost += serverCost;\n        analysis.costByServer[server] = serverCost;\n        \n        // Assume primary model is claude-3-5-sonnet for cost breakdown\n        if (!analysis.costByModel['claude-3-5-sonnet']) {\n          analysis.costByModel['claude-3-5-sonnet'] = 0;\n        }\n        analysis.costByModel['claude-3-5-sonnet'] += serverCost;\n      }\n      \n      // Calculate projected monthly cost\n      const timeframeDays = (Date.now() - startTime.getTime()) / (1000 * 60 * 60 * 24);\n      const dailyAvgCost = analysis.totalCost / Math.max(timeframeDays, 1);\n      analysis.projectedMonthlyCost = dailyAvgCost * 30;\n      \n      // Analyze cost trends (simulate trend analysis)\n      analysis.costTrends = await this.analyzeCostTrends(startTime);\n      \n      // Generate cost optimization opportunities\n      analysis.costOptimizationOpportunities = this.generateCostOptimizations(analysis);\n      \n      return analysis;\n    } catch (error) {\n      piiSafeLogger.error('Failed to analyze costs', {\n        error: error.message\n      });\n      return {\n        totalCost: 0,\n        costByServer: {},\n        costByModel: {},\n        costTrends: { increasing: false, percentageChange: 0 },\n        costOptimizationOpportunities: [],\n        projectedMonthlyCost: 0\n      };\n    }\n  }\n\n  /**\n   * Generate cost optimization recommendations\n   * @param {Object} costAnalysis - Cost analysis data\n   */\n  generateCostOptimizations(costAnalysis) {\n    const opportunities = [];\n    \n    // Find most expensive server\n    const serverCosts = Object.entries(costAnalysis.costByServer)\n      .sort(([,a], [,b]) => b - a);\n    \n    if (serverCosts.length > 0) {\n      const [expensiveServer, cost] = serverCosts[0];\n      if (cost > costAnalysis.totalCost * 0.4) {\n        opportunities.push({\n          type: 'server_optimization',\n          server: expensiveServer,\n          description: `${expensiveServer} accounts for ${((cost / costAnalysis.totalCost) * 100).toFixed(1)}% of total costs`,\n          recommendation: 'Consider optimizing prompts or switching to a more cost-effective model',\n          potentialSavings: cost * 0.2 // Assume 20% potential savings\n        });\n      }\n    }\n    \n    // Find expensive models\n    const modelCosts = Object.entries(costAnalysis.costByModel)\n      .sort(([,a], [,b]) => b - a);\n    \n    for (const [model, cost] of modelCosts) {\n      if (model.includes('opus') && cost > 0) {\n        opportunities.push({\n          type: 'model_optimization',\n          model,\n          description: `Using expensive model: ${model}`,\n          recommendation: 'Consider switching to Claude 3.5 Sonnet for similar quality at lower cost',\n          potentialSavings: cost * 0.6 // Opus to Sonnet can save ~60%\n        });\n      }\n    }\n    \n    // Check for prompt optimization opportunities\n    opportunities.push({\n      type: 'prompt_optimization',\n      description: 'Token usage could be optimized through better prompts',\n      recommendation: 'Implement prompt caching and optimize for conciseness',\n      potentialSavings: costAnalysis.totalCost * 0.15 // 15% potential savings\n    });\n    \n    return opportunities;\n  }\n\n  /**\n   * Check for token usage alerts\n   * @param {string} server - MCP server\n   * @param {string} operation - Operation name\n   * @param {number} tokens - Current token usage\n   */\n  async checkTokenUsageAlert(server, operation, tokens) {\n    try {\n      // Get average token usage for this operation\n      const avgUsage = await this.getAverageTokenUsage(server, operation);\n      \n      if (avgUsage > 0 && tokens > avgUsage * this.alertThresholds.tokenUsageSpike) {\n        const alert = {\n          type: 'token_usage_spike',\n          server,\n          operation,\n          currentTokens: tokens,\n          averageTokens: avgUsage,\n          spikeMultiplier: tokens / avgUsage,\n          timestamp: new Date()\n        };\n        \n        await this.raiseAlert(alert);\n      }\n    } catch (error) {\n      piiSafeLogger.error('Failed to check token usage alert', {\n        error: error.message,\n        server,\n        operation\n      });\n    }\n  }\n\n  /**\n   * Check for quality degradation alerts\n   * @param {string} server - MCP server\n   * @param {string} operation - Operation name\n   * @param {Object} qualityRecord - Quality metrics\n   */\n  async checkQualityAlert(server, operation, qualityRecord) {\n    try {\n      if (qualityRecord.overallQuality < this.alertThresholds.qualityDegradation) {\n        const alert = {\n          type: 'quality_degradation',\n          server,\n          operation,\n          qualityScore: qualityRecord.overallQuality,\n          threshold: this.alertThresholds.qualityDegradation,\n          metrics: {\n            responseTime: qualityRecord.responseTime,\n            accuracy: qualityRecord.accuracy,\n            completion: qualityRecord.completion,\n            userSatisfaction: qualityRecord.userSatisfaction\n          },\n          timestamp: new Date()\n        };\n        \n        await this.raiseAlert(alert);\n      }\n    } catch (error) {\n      piiSafeLogger.error('Failed to check quality alert', {\n        error: error.message,\n        server,\n        operation\n      });\n    }\n  }\n\n  /**\n   * Raise an alert\n   * @param {Object} alert - Alert object\n   */\n  async raiseAlert(alert) {\n    try {\n      // Store alert in database (if models exist)\n      try {\n        if (this.analyticsDB.models && this.analyticsDB.models.MCPAlerts) {\n          await this.analyticsDB.models.MCPAlerts.create({\n            type: alert.type,\n            server: alert.server,\n            operation: alert.operation,\n            severity: this.getAlertSeverity(alert),\n            details: JSON.stringify(alert),\n            resolved: false,\n            timestamp: alert.timestamp\n          });\n        }\n      } catch (dbError) {\n        // Store in memory if database not available\n        piiSafeLogger.warn('MCPAlerts model not found, storing alert in memory only', {\n          error: dbError.message\n        });\n      }\n      \n      // Log alert\n      piiSafeLogger.trackMCPOperation(alert.server, 'alert_raised', {\n        type: alert.type,\n        operation: alert.operation,\n        severity: this.getAlertSeverity(alert)\n      });\n      \n      // Emit alert event\n      this.emit('alert', alert);\n      \n      // In production, this would integrate with alerting systems\n      console.warn(`🚨 MCP Alert: ${alert.type} on ${alert.server}`);\n    } catch (error) {\n      piiSafeLogger.error('Failed to raise alert', {\n        error: error.message,\n        alert\n      });\n    }\n  }\n\n  /**\n   * Get alert severity\n   * @param {Object} alert - Alert object\n   */\n  getAlertSeverity(alert) {\n    switch (alert.type) {\n      case 'token_usage_spike':\n        return alert.spikeMultiplier > 3 ? 'critical' : 'warning';\n      case 'quality_degradation':\n        return alert.qualityScore < 0.6 ? 'critical' : 'warning';\n      case 'cost_threshold':\n        return alert.cost > this.alertThresholds.costThreshold ? 'critical' : 'warning';\n      default:\n        return 'info';\n    }\n  }\n\n  /**\n   * Get average token usage for operation\n   * @param {string} server - MCP server\n   * @param {string} operation - Operation name\n   */\n  async getAverageTokenUsage(server, operation) {\n    try {\n      // Try database first\n      if (this.analyticsDB.models && this.analyticsDB.models.MCPTokenUsage) {\n        const result = await this.analyticsDB.models.MCPTokenUsage.findOne({\n          where: { server, operation },\n          attributes: [\n            [sequelize.fn('AVG', sequelize.col('tokens')), 'avgTokens']\n          ]\n        });\n        \n        if (result) {\n          return parseFloat(result.get('avgTokens')) || 0;\n        }\n      }\n      \n      // Fall back to real-time metrics\n      const metrics = this.realTimeMetrics.get(server);\n      if (metrics && metrics.operationCount > 0) {\n        return metrics.tokenUsage / metrics.operationCount;\n      }\n      \n      return 0;\n    } catch (error) {\n      piiSafeLogger.error('Failed to get average token usage', {\n        error: error.message,\n        server,\n        operation\n      });\n      return 0;\n    }\n  }\n\n  /**\n   * Update real-time metrics\n   * @param {string} server - MCP server\n   * @param {Object} metrics - Metrics to update\n   */\n  updateRealTimeMetrics(server, metrics) {\n    if (!this.realTimeMetrics.has(server)) {\n      this.realTimeMetrics.set(server, {\n        tokenUsage: 0,\n        cost: 0,\n        qualityScore: 0,\n        responseTime: 0,\n        operationCount: 0,\n        lastUpdate: Date.now()\n      });\n    }\n    \n    const current = this.realTimeMetrics.get(server);\n    \n    // Update metrics\n    if (metrics.tokenUsage) current.tokenUsage += metrics.tokenUsage;\n    if (metrics.cost) current.cost += metrics.cost;\n    if (metrics.qualityScore) {\n      current.qualityScore = (current.qualityScore + metrics.qualityScore) / 2;\n    }\n    if (metrics.responseTime) {\n      current.responseTime = (current.responseTime + metrics.responseTime) / 2;\n    }\n    current.operationCount++;\n    current.lastUpdate = Date.now();\n    \n    this.realTimeMetrics.set(server, current);\n  }\n\n  /**\n   * Start real-time monitoring\n   */\n  startRealTimeMonitoring() {\n    // Monitor every 30 seconds\n    setInterval(() => {\n      this.generateRealTimeReport();\n    }, 30000);\n    \n    piiSafeLogger.info('MCP Analytics real-time monitoring started');\n  }\n\n  /**\n   * Generate real-time report\n   */\n  async generateRealTimeReport() {\n    try {\n      const report = {\n        timestamp: new Date().toISOString(),\n        servers: {},\n        totals: {\n          tokenUsage: 0,\n          cost: 0,\n          operations: 0,\n          avgQuality: 0,\n          avgResponseTime: 0\n        }\n      };\n      \n      let serverCount = 0;\n      \n      for (const [server, metrics] of this.realTimeMetrics.entries()) {\n        report.servers[server] = { ...metrics };\n        report.totals.tokenUsage += metrics.tokenUsage;\n        report.totals.cost += metrics.cost;\n        report.totals.operations += metrics.operationCount;\n        report.totals.avgQuality += metrics.qualityScore;\n        report.totals.avgResponseTime += metrics.responseTime;\n        serverCount++;\n      }\n      \n      // Calculate averages\n      if (serverCount > 0) {\n        report.totals.avgQuality /= serverCount;\n        report.totals.avgResponseTime /= serverCount;\n      }\n      \n      this.emit('realTimeReport', report);\n    } catch (error) {\n      piiSafeLogger.error('Failed to generate real-time report', {\n        error: error.message\n      });\n    }\n  }\n\n  /**\n   * Get timeframe in milliseconds\n   * @param {string} timeframe - 'hour', 'day', 'week', 'month'\n   */\n  getTimeframeMs(timeframe) {\n    const timeframes = {\n      hour: 60 * 60 * 1000,\n      day: 24 * 60 * 60 * 1000,\n      week: 7 * 24 * 60 * 60 * 1000,\n      month: 30 * 24 * 60 * 60 * 1000\n    };\n    \n    return timeframes[timeframe] || timeframes.day;\n  }\n\n  /**\n   * Generate recommendations based on analytics\n   * @param {Object} report - Health report\n   */\n  generateRecommendations(report) {\n    const recommendations = [];\n    \n    // Token efficiency recommendations\n    if (report.summary.tokenEfficiency.tokenEfficiencyScore < 80) {\n      recommendations.push({\n        category: 'efficiency',\n        priority: 'high',\n        title: 'Optimize Token Usage',\n        description: 'Token efficiency is below optimal levels',\n        actions: [\n          'Review and optimize prompts for conciseness',\n          'Implement prompt caching where applicable',\n          'Consider switching to more efficient models'\n        ],\n        impact: 'Could reduce costs by 15-30%'\n      });\n    }\n    \n    // Quality recommendations\n    if (report.summary.responseQuality.overallQualityScore < 0.85) {\n      recommendations.push({\n        category: 'quality',\n        priority: 'medium',\n        title: 'Improve Response Quality',\n        description: 'Response quality is below target levels',\n        actions: [\n          'Review and update AI prompts',\n          'Implement quality feedback loops',\n          'Add more comprehensive testing'\n        ],\n        impact: 'Improve user satisfaction and accuracy'\n      });\n    }\n    \n    // Cost optimization recommendations\n    if (report.summary.costAnalysis.projectedMonthlyCost > 1000) {\n      recommendations.push({\n        category: 'cost',\n        priority: 'high',\n        title: 'Optimize Operating Costs',\n        description: 'Monthly costs are projected to exceed budget',\n        actions: [\n          'Implement cost monitoring alerts',\n          'Consider model alternatives for non-critical operations',\n          'Optimize high-cost operations'\n        ],\n        impact: `Potential savings: $${(report.summary.costAnalysis.projectedMonthlyCost * 0.2).toFixed(2)}/month`\n      });\n    }\n    \n    return recommendations;\n  }\n\n  // Helper methods with mock implementations\n  async calculateUptimeStats(startTime) {\n    return {\n      uptimePercentage: 99.5,\n      totalOperations: 1234,\n      successRate: 0.995\n    };\n  }\n  \n  async analyzeQualityTrends(startTime) {\n    return {\n      qualityImproving: true,\n      responseTimeImproving: true\n    };\n  }\n  \n  async analyzeCostTrends(startTime) {\n    return {\n      increasing: false,\n      percentageChange: -5.2\n    };\n  }\n  \n  async getServerDetailedMetrics(serverKey, startTime) {\n    const metrics = this.realTimeMetrics.get(serverKey) || {\n      tokenUsage: 0,\n      cost: 0,\n      operationCount: 0,\n      qualityScore: 0.85,\n      responseTime: 1200\n    };\n    \n    return {\n      server: serverKey,\n      totalTokens: metrics.tokenUsage,\n      totalCost: metrics.cost,\n      operations: metrics.operationCount,\n      qualityScore: metrics.qualityScore,\n      avgResponseTime: metrics.responseTime,\n      uptime: 99.5,\n      errorRate: 0.005\n    };\n  }\n  \n  async analyzeTrends(startTime) {\n    return {\n      tokenUsage: { trend: 'stable', change: 2.3 },\n      quality: { trend: 'improving', change: 5.1 },\n      cost: { trend: 'decreasing', change: -3.7 },\n      responseTime: { trend: 'improving', change: -8.2 }\n    };\n  }\n  \n  async getAlertsSummary(startTime) {\n    return {\n      total: 3,\n      critical: 0,\n      warning: 2,\n      info: 1,\n      resolved: 1\n    };\n  }\n\n  /**\n   * Get real-time metrics for a specific server\n   * @param {string} server - Server name\n   */\n  getRealTimeMetrics(server) {\n    return this.realTimeMetrics.get(server) || null;\n  }\n\n  /**\n   * Get all real-time metrics\n   */\n  getAllRealTimeMetrics() {\n    return Object.fromEntries(this.realTimeMetrics);\n  }\n\n  /**\n   * Reset metrics for a server\n   * @param {string} server - Server name\n   */\n  resetServerMetrics(server) {\n    this.realTimeMetrics.delete(server);\n    piiSafeLogger.info(`Metrics reset for server: ${server}`);\n  }\n\n  /**\n   * Get analytics summary\n   */\n  getAnalyticsSummary() {\n    const summary = {\n      totalServers: this.realTimeMetrics.size,\n      totalOperations: 0,\n      totalCost: 0,\n      totalTokens: 0,\n      avgQuality: 0,\n      avgResponseTime: 0\n    };\n    \n    let serverCount = 0;\n    for (const metrics of this.realTimeMetrics.values()) {\n      summary.totalOperations += metrics.operationCount;\n      summary.totalCost += metrics.cost;\n      summary.totalTokens += metrics.tokenUsage;\n      summary.avgQuality += metrics.qualityScore;\n      summary.avgResponseTime += metrics.responseTime;\n      serverCount++;\n    }\n    \n    if (serverCount > 0) {\n      summary.avgQuality /= serverCount;\n      summary.avgResponseTime /= serverCount;\n    }\n    \n    return summary;\n  }\n}\n\n// Singleton instance\nexport const mcpAnalytics = new MCPAnalytics();\n\nexport default MCPAnalytics;